{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c744de79-6aaf-46ca-946a-29b4d094d8c2",
   "metadata": {},
   "source": [
    "### 네이버 뉴스 크롤링 최종\n",
    "- 섹션 추출 ~ sub 페이지 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdafd8b5-0964-4fe8-86bc-c247dc37fb85",
   "metadata": {},
   "source": [
    "#### (1) + (2) + (3)\n",
    "(1) 섹션 메뉴와 섹션별 url 추출      \n",
    "    - section, link  \n",
    "    - section_menu_df  \n",
    "(2) 섹션별 헤드라인(topic) 제목과 url 추출  \n",
    "    - topic, url, section  \n",
    "    - all_topic_df  \n",
    "(3) 섹션 헤드라인별 sub 페이지 기사 내용 추출  \n",
    "    - 언론사, 제목, 날짜/시간, 기사내용, 작성자  \n",
    "    - paper, title, datetime, writing, writer  \n",
    "    - all_sub_news_info_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39af4d6c-0857-480f-99ec-6385ebb1484d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee9729f-123e-4e59-812e-b4c9af7e113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity=\"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b4a2ff1-0f36-4820-80b6-fbc32357ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831bff05-6c3d-4059-8f85-d739700ebf6f",
   "metadata": {},
   "source": [
    "#### (1) 네이버 뉴스 섹션 메뉴와 url 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8579c8e8-c552-4461-aab4-066c98e7a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://news.naver.com/'\n",
    "html = urlopen(url)\n",
    "# 파서 객체 생성\n",
    "bs_obj = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7cf80f1-4a91-4e42-a12b-269fc90344ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수집된 데이터로 df 구성\n",
    "section = []\n",
    "link = []\n",
    "ul = bs_obj.find('ul', {'class':'Nlnb_menu_list'})\n",
    "lis = ul.findAll('li')\n",
    "# 하나씩 추출해서 리스트에 추가\n",
    "for li in lis:\n",
    "    a_tag = li.find('a')\n",
    "    section.append(a_tag.text)\n",
    "    link.append(a_tag['href'])\n",
    "section_menu_df = pd.DataFrame({'section':section, 'link':link })\n",
    "section_menu_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b031d15-90cf-486b-b8f3-caf0cde362c2",
   "metadata": {},
   "source": [
    "#### (2) 섹션별 헤드라인(topic) 제목과 url 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2da50bd2-33fb-4321-a68d-723f39b81bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 섹션에 대한 모든 topic 크롤링하는 함수 \n",
    "# 함수로 구성\n",
    "# 섹션과 url 전달 받아서, 헤드라인 topic의 제목, 링크, 섹션을 추출하여\n",
    "# 딕셔너리로 만들어 반환하는 함수\n",
    "def get_topic(url, section):\n",
    "    # {1} 요청 후 응답 받고 파싱객체 생성\n",
    "    # 요청 시 전달할 header : 딕셔너리루 구성(key:value, 모두 문자열)\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"}\n",
    "    res = requests.get(url, headers=headers) # 응답객체를 반환 :해당 객체가 복잡하므로 bs4가 인식할 수 없음\n",
    "    html = res.text\n",
    "\n",
    "    # 파싱 객체\n",
    "    bs_obj=BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # 파싱\n",
    "    lis = bs_obj.select(\"ul.sa_list div.sa_text\")\n",
    "\n",
    "    # 컨텐츠 추출\n",
    "    title=[]\n",
    "    titleLink=[]\n",
    "    for li in lis:\n",
    "        try:\n",
    "            title.append(li.a.text.replace(\"\\n\", \"\"))\n",
    "            titleLink.append(li.a[\"href\"])\n",
    "        except:\n",
    "            print(\"예외 발생\")\n",
    "    return ({\"기사제목\":title,\"기사링크\":titleLink, \"분류\":section})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cab7d4b-5d81-4b66-9ef8-94d9291e0a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수집된 데이터로 df 구성\n",
    "all_topic_df = pd.DataFrame({\n",
    "                \"기사제목\":[],\n",
    "                \"기사링크\":[],\n",
    "                \"분류\":[],\n",
    "                })\n",
    "# 모든 섹션에 대한 모든 topic 데이터프레임 생성\n",
    "for i in range(1, 7):\n",
    "    tmpDF=pd.DataFrame(get_topic(section_menu_df[\"link\"][i], section_menu_df[\"menu\"][i]))\n",
    "    all_topic_df = pd.concat([all_topic_df, tmpDF], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8744392-a493-49d4-b369-2d47188062a0",
   "metadata": {},
   "source": [
    "#### (3) 섹션 헤드라인별 sub 페이지 기사 내용 추출\n",
    "- 언론사, 제목, 날짜/시간, 기사내용, 작성자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "618a9855-e6ce-4063-a621-51cb6b73ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub 페이지 기사 내용 추출하는 함수\n",
    "def get_sub_news_info(url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"}\n",
    "        res = requests.get(url, headers=headers)\n",
    "        html=res.text\n",
    "        bs_obj=BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # 데이터 자장용 빈리스트 생성\n",
    "        paper_list, title_list, datetime_list, writing_list, writer_list = [[]for _ in range(5)]\n",
    "        # 데이터 추출\n",
    "        # 언론사\n",
    "        paper_list.append(bs_obj.find(\"a\", {\"class\":\"media_end_head_top_logo\"}).select_one(\"img\")[\"title\"])\n",
    "        # 기사제목\n",
    "        title_list.append(bs_obj.select_one(\"h2#title_area span\").text)\n",
    "        # 기사입력 날짜 및 시간\n",
    "        datetime_list.append(bs_obj.select_one(\".media_end_head_info_datestamp_time._ARTICLE_DATE_TIME\").text)\n",
    "        # 기사 내용\n",
    "        writing_list.append(bs_obj.select_one(\"#dic_area\").text.replace(\"\\n\", \"\"))\n",
    "        # 작성 기자\n",
    "        if (bs_obj.select_one(\".byline_s\") == None):\n",
    "            writer_list.append(\"없음\")\n",
    "        else:\n",
    "            writer_list.append(bs_obj.select_one(\".byline_s\").text)\n",
    "    except Exception as e:\n",
    "        print(\"오류 발생 :\", e)\n",
    "    \n",
    "    # 지정된 list 내 원소수 일치여부 확인\n",
    "    print(len(paper_list), len(title_list), len(datetime_list), len(writing_list), len(writer_list))\n",
    "    sub_new_dict = {\"paper\":paper_list, \n",
    "                                 \"title\":title_list, \n",
    "                                 \"datetime\":datetime_list, \n",
    "                                 \"writing\":writing_list, \n",
    "                                 \"writer\":writer_list}\n",
    "    return sub_new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ba10f06-a391-4b53-84e0-c723f921f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 sub 페이지 내용 저장하는 데이터프레임 생성해서 최종 데이터프레임 생성\n",
    "all_sub_news_info_df = pd.DataFrame({\"paper\":[], \n",
    "                                 \"title\":[], \n",
    "                                 \"datetime\":[], \n",
    "                                 \"writing\":[], \n",
    "                                 \"writer\":[]\n",
    "                                 })\n",
    "all_sub_news_info_df\n",
    "for url in all_topic_df[\"기사링크\"]:\n",
    "    tmp_df = pd.DataFrame(get_sub_news_info(url))\n",
    "    all_sub_news_info_df=pd.concat([all_sub_news_info_df, tmp_df], axis = 0, ignore_index=True)\n",
    "# 여기서 나오는 에러값은 요소의 길이중 1이 아닌 것이 있다는 얘기, writer가 공란인 경우가 있음\n",
    "all_sub_news_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e1cbc7b-ad02-47a3-9fe6-4a9ecfcffbea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_sub_news_info_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 최종 결과 파일로 저장\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mall_sub_news_info_df\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./crawl_data/naver_sub_news_section_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_sub_news_info_df' is not defined"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "today = pd.to_datetime(datetime.datetime.now()).strftime(\"%Y-%M-%d|%H:%m\")\n",
    "all_sub_news_info_df.to_csv(\"./crawl_data/naver_sub_news_section_{today}.csv\", index=0, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d20f66-11da-4e2d-8609-0c96d00ea0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e98bde-a29e-4a98-b26c-563748ce1965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“scrapSrc",
   "language": "python",
   "name": "scraping_source"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
