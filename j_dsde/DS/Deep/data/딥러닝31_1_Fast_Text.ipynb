{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eabd5e0a-00fb-4e0e-a56c-89fdcdc4f85f",
   "metadata": {},
   "source": [
    "### FastText\n",
    "- 페이스북에서 개발\n",
    "- Word2Vect 확장 모듈(임베딩모델)\n",
    "- 워드 임베딩에 N-gram 적용\n",
    "- word2vect와 차이점\n",
    "    - word2vect : 단어(token)는 쪼개질 수 없는 단위로 생각\n",
    "    - fasttext : 하나의 단어 안에 여러 단어들이 존재하는 것으로 간주\n",
    "        - 내부 단어, 서부워드를 고려해서 학습\n",
    "- fasttext 특징\n",
    "    - 내부 단어 학습(철자별로 해버림)\n",
    "    - 모르는 단어에 대한 대응\n",
    "    - 빈도수가 적었던 단어에 대한 대응"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db044a91-1325-4f18-89ba-a2b36f57b656",
   "metadata": {},
   "source": [
    "### 내부 단어 학습\n",
    "- N-gram 방식을 도입 : 단어에 문자 단위로 적용\n",
    "- 'hello' : bi-gram 적용 (he, el, ll, lo) 분리하고 벡터로 생성\n",
    "- 'apple' : n=3 (ap, app, ppl, ple, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121accde-ecd4-4e9d-ad38-c46d9dac5258",
   "metadata": {},
   "source": [
    "### 모르는 단어 : oov(out of vocabulary)에 대한 대응\n",
    "- birthplace : oov 라고 한다면\n",
    "    - birth // place 나눠서 사전에서 탐색 >> 두 단어 임베딩벡터 결합해서 birthplace 얻을 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb4d54e-2382-4fc7-9328-7efd11696365",
   "metadata": {},
   "source": [
    "### 빈도수가 적은 단어\n",
    "- 오타인 경우 빈도수가 적다고 가정\n",
    "- 오타인 단어를 쪼개서 오타가 아닌 부분을 gram으로 사용해서 사전에서 확인 >> 해당 임베딩을 사용했을 때, 성능 향상\n",
    "- birth >> 오타 : bith >> bi, th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7718e22b-eda0-4425-9ad1-82fb39f3e120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98db7743-1c4a-47d8-9e87-2dcaed333fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 \n",
    "from nltk.tokenize import  sent_tokenize, word_tokenize\n",
    "from lxml import etree\n",
    "import re\n",
    "\n",
    "targetXML = open('./data/ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "target_text = etree.parse(targetXML)\n",
    "\n",
    "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "\n",
    "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분 제거.\n",
    "# 해당 코드는 괄호로 구성된 내용을 제거.\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
    "sent_text = sent_tokenize(content_text)\n",
    "\n",
    "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "     normalized_text.append(tokens)\n",
    "\n",
    "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_ENV",
   "language": "python",
   "name": "dlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
