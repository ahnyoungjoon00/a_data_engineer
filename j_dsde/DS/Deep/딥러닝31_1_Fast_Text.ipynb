{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eabd5e0a-00fb-4e0e-a56c-89fdcdc4f85f",
   "metadata": {},
   "source": [
    "### FastText\n",
    "- 페이스북에서 개발\n",
    "- Word2Vect 확장 모듈(임베딩모델)\n",
    "- 워드 임베딩에 N-gram 적용\n",
    "- word2vect와 차이점\n",
    "    - word2vect : 단어(token)는 쪼개질 수 없는 단위로 생각\n",
    "    - fasttext : 하나의 단어 안에 여러 단어들이 존재하는 것으로 간주\n",
    "        - 내부 단어, 서부워드를 고려해서 학습\n",
    "- fasttext 특징\n",
    "    - 내부 단어 학습(철자별로 해버림)\n",
    "    - 모르는 단어에 대한 대응\n",
    "    - 빈도수가 적었던 단어에 대한 대응"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db044a91-1325-4f18-89ba-a2b36f57b656",
   "metadata": {},
   "source": [
    "### 내부 단어 학습\n",
    "- N-gram 방식을 도입 : 단어에 문자 단위로 적용\n",
    "- 'hello' : bi-gram 적용 (he, el, ll, lo) 분리하고 벡터로 생성\n",
    "- 'apple' : n=3 (ap, app, ppl, ple, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121accde-ecd4-4e9d-ad38-c46d9dac5258",
   "metadata": {},
   "source": [
    "### 모르는 단어 : oov(out of vocabulary)에 대한 대응\n",
    "- birthplace : oov 라고 한다면\n",
    "    - birth // place 나눠서 사전에서 탐색 >> 두 단어 임베딩벡터 결합해서 birthplace 얻을 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb4d54e-2382-4fc7-9328-7efd11696365",
   "metadata": {},
   "source": [
    "### 빈도수가 적은 단어\n",
    "- 오타인 경우 빈도수가 적다고 가정\n",
    "- 오타인 단어를 쪼개서 오타가 아닌 부분을 gram으로 사용해서 사전에서 확인 >> 해당 임베딩을 사용했을 때, 성능 향상\n",
    "- birth >> 오타 : bith >> bi, th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7718e22b-eda0-4425-9ad1-82fb39f3e120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98db7743-1c4a-47d8-9e87-2dcaed333fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 \n",
    "from nltk.tokenize import  sent_tokenize, word_tokenize\n",
    "from lxml import etree\n",
    "import re\n",
    "\n",
    "targetXML = open('./data/ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "target_text = etree.parse(targetXML)\n",
    "\n",
    "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "\n",
    "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분 제거.\n",
    "# 해당 코드는 괄호로 구성된 내용을 제거.\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
    "sent_text = sent_tokenize(content_text)\n",
    "\n",
    "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "     normalized_text.append(tokens)\n",
    "\n",
    "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f31960ce-571c-4126-8172-30bcabe9d64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273424"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1118f2-c365-44c4-bbba-982895dc98af",
   "metadata": {},
   "source": [
    "### word2vec와 fasttext 차이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c04902a5-03bc-4bc7-b6cd-2eb0585fb127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result 데이터로 학습한 word2vec 모델 생성\n",
    "from gensim.models import Word2Vec, FastText\n",
    "model_wv = Word2Vec(sentences=result, vector_size =100, window=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56d6593f-f88d-4dbc-82e9-5998fbd04c4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('martin', 0.8728253841400146),\n",
       " ('luther', 0.8509300351142883),\n",
       " ('james', 0.8490146398544312),\n",
       " ('thomas', 0.8405089378356934),\n",
       " ('john', 0.8357966542243958),\n",
       " ('president', 0.828530490398407),\n",
       " ('dr', 0.8105995059013367),\n",
       " ('charles', 0.8077185750007629),\n",
       " ('george', 0.8053943514823914),\n",
       " ('alexander', 0.8024201393127441)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wv.wv[\"king\"]\n",
    "model_wv.wv.most_similar(\"king\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d5aa370-6d2b-4096-8aaa-7980eaf1d852",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'electrofishing' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 단어 사전에 없는 단어(oov) : word2vec에선 에러 발생 >> KeyError: \"Key 'electrofishing' not present in vocabulary\"\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_wv\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mmost_similar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melectrofishing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DLenv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_mean_vector(keys, weight, pre_normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, post_normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ignore_missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[0;32m    844\u001b[0m ]\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DLenv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    516\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[1;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    521\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m/\u001b[39m total_weight\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'electrofishing' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "# 단어 사전에 없는 단어(oov) : word2vec에선 에러 발생 >> KeyError: \"Key 'electrofishing' not present in vocabulary\"\n",
    "model_wv.wv.most_similar(\"electrofishing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df787e38-b45c-458d-868d-3cf995d4cbf1",
   "metadata": {},
   "source": [
    "### FastText\n",
    "- 단어들에 n-gram 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81d70ba5-8443-41a3-9a06-51dc640c05b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = FastText(result, vector_size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fb4a67f-de81-47d5-9976-3d47ebb3bd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('viking', 0.9769628047943115),\n",
       " ('biking', 0.9735829830169678),\n",
       " ('joking', 0.9715903997421265),\n",
       " ('hiking', 0.9700593948364258),\n",
       " ('spiking', 0.9686288833618164),\n",
       " ('pecking', 0.9664033651351929),\n",
       " ('poking', 0.9660462141036987),\n",
       " ('cranking', 0.9644487500190735),\n",
       " ('lacking', 0.9639366269111633),\n",
       " ('hawking', 0.9636201858520508)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.wv.most_similar(\"king\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51223120-6ea5-4768-a6f3-02991d7b1f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fishing', 0.9170586466789246),\n",
       " ('licensing', 0.9080235958099365),\n",
       " ('flashing', 0.9062708616256714),\n",
       " ('flushing', 0.9054815769195557),\n",
       " ('flourishing', 0.9031326770782471),\n",
       " ('refreshing', 0.8977901339530945),\n",
       " ('smashing', 0.8968352675437927),\n",
       " ('vanishing', 0.8957969546318054),\n",
       " ('operating', 0.8953908085823059),\n",
       " ('transplanting', 0.8943449854850769)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.wv.most_similar(\"electrofishing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0e7b9e-5c9c-4d06-b32b-9672f2f411e0",
   "metadata": {},
   "source": [
    "### 한국어 fasttext\n",
    "- 한글의 경우에도 oov문제를 해결하기 위해 fasttext 적용 시도\n",
    "- 음절 단위로 n-gram 적용 ( word2vec)\n",
    "    - n=3, \"자연어처리\"에 대한 n-gram\n",
    "    - <자연, 자연어, 연어처, 어처리, 처리> 와 같은 방식으로 n-gram이 적용\n",
    "- 자모단위 ( fasttext)\n",
    "    - 초성,중성,종성 단위로 임베딩\n",
    "    - 오타가 많은 데이터에서 더 강한 임베딩 성능 기대 가능\n",
    "    - \"자연어처리\"에 대한 n-gram : 자모단위\n",
    "    - n=3, <ㅈ ㅏ, ㅈ ㅏ _(받침이 없는 경우 받침 없는 걸 표현), ㅏ _ ㅇ, ㅇ ㅕ ㄴ,\n",
    "            ㅕ ㄴ ㅇ, ...> 의 형태로\n",
    "- 한글 자모단위 처리 패키지\n",
    "    - pip install hgtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04cdf3c4-9ea0-4164-b50e-c33297170fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hgtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c59de34d-4626-4497-ba19-ea0c08d295d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한글 확인 \n",
    "hgtk.checker.is_hangul(\"ㄱ\")\n",
    "# hgtk.checker.is_hangul(\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79c45066-a03a-4287-9fcd-55824ecde303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ㄴ', 'ㅏ', 'ㅁ')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hgtk.letter.decompose(\"letter\") # 한글 아니라 에러\n",
    "hgtk.letter.decompose(\"남\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f613b9d9-bd94-4652-9fef-360477a3898a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ㅇ', 'ㅕ', '')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cho, jung, jong = hgtk.letter.decompose(\"여\")\n",
    "cho, jung, jong "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89eef831-8e69-453d-b739-33862f6c3a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'나'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 자모단위 결합 함수\n",
    "hgtk.letter.compose(\"ㄴ\", \"ㅏ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa2f48bb-ead9-4de3-8255-5554ea3557a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결합할 수 없는 상황에서는 에러 : NotHangulException: No valid Hangul character index\n",
    "# hgtk.letter.compose(\"ㄴ\", \"ㅁ\", \"ㅇ\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ce5108-0768-42ed-9ac7-ea9ed6cad159",
   "metadata": {},
   "source": [
    "### 네이버 쇼핑 리뷰 데이터 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22e33d81-aafa-497b-b4df-58745873940b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>배공빠르고 굿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>택배가 엉망이네용 저희집 밑에층에 말도없이 놔두고가고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>아주좋아요 바지 정말 좋아서2개 더 구매했어요 이가격에 대박입니다. 바느질이 조금 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다. 전...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>민트색상 예뻐요. 옆 손잡이는 거는 용도로도 사용되네요 ㅎㅎ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ratings                                             review\n",
       "0        5                                            배공빠르고 굿\n",
       "1        2                      택배가 엉망이네용 저희집 밑에층에 말도없이 놔두고가고\n",
       "2        5  아주좋아요 바지 정말 좋아서2개 더 구매했어요 이가격에 대박입니다. 바느질이 조금 ...\n",
       "3        2  선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다. 전...\n",
       "4        5                  민트색상 예뻐요. 옆 손잡이는 거는 용도로도 사용되네요 ㅎㅎ"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "total_data = pd.read_table(\"./data/ratings_total.txt\", names= [\"ratings\", \"review\"])\n",
    "total_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d23e3f6-5ba1-44ca-98c0-baf1cf6f763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 특정 단어를 초성,중성,종성으로 나누는함수\n",
    "\n",
    "# def word_to_jamo(token):\n",
    "#   def to_special_token(jamo):\n",
    "#     if not jamo:\n",
    "#       return '-'\n",
    "#     else:\n",
    "#       return jamo\n",
    "\n",
    "#   decomposed_token = ''\n",
    "#   for char in token:\n",
    "#     try:\n",
    "#       # char(음절)을 초성, 중성, 종성으로 분리\n",
    "#       cho, jung, jong = hgtk.letter.decompose(char)\n",
    "\n",
    "#       # 자모가 빈 문자일 경우 특수문자 -로 대체\n",
    "#       cho = to_special_token(cho)\n",
    "#       jung = to_special_token(jung)\n",
    "#       jong = to_special_token(jong)\n",
    "#       decomposed_token = decomposed_token + cho + jung + jong\n",
    "\n",
    "#     # 만약 char(음절)이 한글이 아닐 경우 자모를 나누지 않고 추가\n",
    "#     except Exception as exception:\n",
    "#       if type(exception).__name__ == 'NotHangulException':\n",
    "#         decomposed_token += char\n",
    "    \n",
    "#   # 단어 토큰의 자모 단위 분리 결과를 추가\n",
    "#   return decomposed_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be804c33-b947-4824-97c1-b4ade44f4e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 단어를 초, 중, 종성으로 나누는 사용자 정의 함수\n",
    "def word_to_jamo(token) :\n",
    "    def to_special_token(jamo) :\n",
    "        if not jamo :\n",
    "            return \"-\"\n",
    "        else :\n",
    "            return jamo\n",
    "    decomposed_token = \"\"\n",
    "    for char in token :\n",
    "        try :\n",
    "            cho, jung, jong = hgtk.letter.decompose(char) # 받침이 없으면 jong은 \"\"가 입력됨\n",
    "            # 자모가 빈 문자열일 경우, 특수문자 \"-\"로 대체\n",
    "            cho = to_special_token(cho)\n",
    "            jung = to_special_token(jung)\n",
    "            jong = to_special_token(jong)\n",
    "            decomposed_token = decomposed_token + cho + jung + jong\n",
    "        except Exception as exception :\n",
    "            if type(exception).__name__ == \"NotHangulException\":\n",
    "                decomposed_token += char\n",
    "    return decomposed_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0321a42b-bfe8-4f16-a6af-fee7be6b6360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ㄴㅏㅁㄷㅗㅇㅅㅐㅇ'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_jamo(\"남동생\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "241b8c00-a95c-4892-b90b-7551ee222aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ㅇㅕ-ㄷㅗㅇㅅㅐㅇ'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_jamo(\"여동생\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8a91cb1-4e7a-42bc-b362-6720c488a274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['선물',\n",
       " '용',\n",
       " '으로',\n",
       " '빨리',\n",
       " '받아서',\n",
       " '전달',\n",
       " '했어야',\n",
       " '하는',\n",
       " '상품',\n",
       " '이었는데',\n",
       " '머그컵',\n",
       " '만',\n",
       " '와서',\n",
       " '당황',\n",
       " '했습니다',\n",
       " '.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "okt.morphs(\"선물용으로 빨리 받아서 전달 했어야 하는 상품이었는데 머그컵만 와서 당황했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9595df2c-dfb6-4331-8487-32d0a83318be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_by_jamo(sentence) :\n",
    "    return [word_to_jamo(token) for token in okt.morphs(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd95015b-e299-4fc9-a04b-f585b87df993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ㅅㅓㄴㅁㅜㄹ', 'ㅇㅛㅇ', 'ㅇㅡ-ㄹㅗ-', 'ㅃㅏㄹㄹㅣ-', 'ㅂㅏㄷㅇㅏ-ㅅㅓ-', 'ㅈㅓㄴㄷㅏㄹ', 'ㅎㅐㅆㅇㅓ-ㅇㅑ-', 'ㅎㅏ-ㄴㅡㄴ', 'ㅅㅏㅇㅍㅜㅁ', 'ㅇㅣ-ㅇㅓㅆㄴㅡㄴㄷㅔ-', 'ㅁㅓ-ㄱㅡ-ㅋㅓㅂ', 'ㅁㅏㄴ', 'ㅇㅘ-ㅅㅓ-', 'ㄷㅏㅇㅎㅘㅇ', 'ㅎㅐㅆㅅㅡㅂㄴㅣ-ㄷㅏ-', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"선물용으로 빨리 받아서 전달 했어야 하는 상품이었는데 머그컵만 와서 당황했습니다.\"\n",
    "print(tokenize_by_jamo(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2baee9da-1608-40dc-9f97-6e8eacafc83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 200000/200000 [1:05:18<00:00, 51.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1h 5min 40s\n",
      "Wall time: 1h 5min 18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenized_data = [] # 리부 전체 문장 토큰 저장할 리스트\n",
    "for sample in tqdm(total_data[\"review\"].to_list()):\n",
    "    tokenized_sample = tokenize_by_jamo(sample)\n",
    "    tokenized_data.append(tokenized_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "099ee919-e225-4122-9180-3fc45a50676c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f541133b-92c9-489a-8701-f8c46265f528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ㅂㅐ-ㄱㅗㅇ', 'ㅃㅏ-ㄹㅡ-ㄱㅗ-', 'ㄱㅜㅅ']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8efb7b8-d277-4e74-bcbd-8119cdad898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어로 복원해주는 함수 \n",
    "\n",
    "def jamo_to_word(jamo_sequence):\n",
    "  tokenized_jamo = []\n",
    "  index = 0\n",
    "  \n",
    "  # 1. 초기 입력\n",
    "  # jamo_sequence = 'ㄴㅏㅁㄷㅗㅇㅅㅐㅇ'\n",
    "\n",
    "  while index < len(jamo_sequence):\n",
    "    # 문자가 한글(정상적인 자모)이 아닐 경우\n",
    "    if not hgtk.checker.is_hangul(jamo_sequence[index]):\n",
    "      tokenized_jamo.append(jamo_sequence[index])\n",
    "      index = index + 1\n",
    "\n",
    "    # 문자가 정상적인 자모라면 초성, 중성, 종성을 하나의 토큰으로 간주.\n",
    "    else:\n",
    "      tokenized_jamo.append(jamo_sequence[index:index + 3])\n",
    "      index = index + 3\n",
    "\n",
    "  # 2. 자모 단위 토큰화 완료\n",
    "  # tokenized_jamo : ['ㄴㅏㅁ', 'ㄷㅗㅇ', 'ㅅㅐㅇ']\n",
    "  \n",
    "  word = ''\n",
    "  try:\n",
    "    for jamo in tokenized_jamo:\n",
    "\n",
    "      # 초성, 중성, 종성의 묶음으로 추정되는 경우\n",
    "      if len(jamo) == 3:\n",
    "        if jamo[2] == \"-\":\n",
    "          # 종성이 존재하지 않는 경우\n",
    "          word = word + hgtk.letter.compose(jamo[0], jamo[1])\n",
    "        else:\n",
    "          # 종성이 존재하는 경우\n",
    "          word = word + hgtk.letter.compose(jamo[0], jamo[1], jamo[2])\n",
    "      # 한글이 아닌 경우\n",
    "      else:\n",
    "        word = word + jamo\n",
    "    # 복원 중(hgtk.letter.compose) 에러 발생 시 초기 입력 리턴.\n",
    "  # 복원이 불가능한 경우 예시) 'ㄴ!ㅁㄷㅗㅇㅅㅐㅇ'\n",
    "  except Exception as exception:  \n",
    "    if type(exception).__name__ == 'NotHangulException':\n",
    "      return jamo_sequence\n",
    "\n",
    "  # 3. 단어로 복원 완료\n",
    "  # word : '남동생'\n",
    "\n",
    "  return word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b0af9-6ecc-4290-9a2c-d04bf4978dc6",
   "metadata": {},
   "source": [
    "### 한글 FastText 임베딩 모델 생성\n",
    "- pip install fasttext-wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a839e2da-0d42-4f73-9d9d-268270db9380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c86bab71-b975-48be-865c-5fcf02f27ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 200000/200000 [00:00<00:00, 356555.42 line/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenized_data(list)를 말뭉치(텍스트 집합)으로 다시 구성해야함 >> 즉, list 요소를 txt파일로 저장\n",
    "with open(\"./data/tokenized_data.txt\", \"w\", encoding=\"UTF8\") as out :\n",
    "    for line in tqdm(tokenized_data, unit=\" line\"):\n",
    "        out.write(\" \".join(line) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "db773797-5bc1-4ead-8083-48271de91143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext.train_unsupervised >> 말뭉치를 주입받아서 fasttext 방식으로 임베딩벡터 생성하는 모듈\n",
    "model_ft_kor = fasttext.train_unsupervised(\"./data/tokenized_data.txt\", model=\"cbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce0a1bce-b717-4b59-8f95-c8859e4477c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.352271  , -0.01096702,  0.06772958, -0.49084365, -0.91934633,\n",
       "        0.67120206, -0.06407845,  0.90819466, -0.49045715, -0.02728405,\n",
       "       -0.51844776, -0.9557729 , -0.0447108 , -0.424039  ,  0.45476815,\n",
       "        0.5911588 ,  0.76276433, -0.3132864 , -0.04146839, -0.7162987 ,\n",
       "       -0.07241749, -1.371207  , -0.31391716, -0.73100954, -0.21320039,\n",
       "        1.0922103 ,  0.31125253,  0.4317013 ,  0.9374813 ,  0.19062869,\n",
       "       -0.36007923,  0.5329514 , -0.5572704 ,  0.33488792, -0.6640924 ,\n",
       "        0.47277847, -0.23200971,  1.7224989 ,  0.5872937 ,  0.08474042,\n",
       "       -1.3061486 , -0.07197918, -0.07680016,  0.8138374 ,  1.3192397 ,\n",
       "        0.12671499, -0.22916922,  0.9782795 ,  0.6477222 , -0.28950304,\n",
       "       -0.3767516 ,  0.28815356,  0.43184236, -0.19069389,  0.3739281 ,\n",
       "       -0.1409797 ,  0.33875376, -0.14300728,  0.6732585 , -0.6241671 ,\n",
       "        0.6535004 ,  0.2698578 ,  0.8960673 ,  0.47058603, -0.85397184,\n",
       "       -0.34458378,  0.43461263, -0.23304343, -0.63023025, -0.6338231 ,\n",
       "        0.5585066 , -0.6815549 ,  0.3013359 ,  0.03892016,  0.7766942 ,\n",
       "        0.7956397 ,  0.39256763,  0.29582497,  0.50663185,  0.47282147,\n",
       "        0.18485707,  0.38229188, -0.6922162 ,  1.388716  ,  0.24543473,\n",
       "       -0.05521877, -0.2237157 ,  0.6290598 , -0.42905018,  0.7544756 ,\n",
       "       -0.15668029,  0.2106686 , -0.28891003, -0.01996476,  0.00259322,\n",
       "       -0.6072628 , -0.01718482,  0.6117771 ,  0.12084786,  0.23105294],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_ft_kor은 초/중/종성이 펼쳐진 데이터로 학습을 진행 >> 임베딩벡터값 확인하기 위해서는 초/중/종성이 펼쳐진 데이터로 확인 \n",
    "model_ft_kor[word_to_jamo(\"남동생\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af1a2a46-0b55-4c3f-b1e6-54b81f1250aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ㄴㅏㅁㄷㅗㅇㅅㅐㅇ'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_jamo(\"남동생\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a12f2cf9-e07e-46b4-9178-fc7c386bd3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.352271  , -0.01096702,  0.06772958, -0.49084365, -0.91934633,\n",
       "        0.67120206, -0.06407845,  0.90819466, -0.49045715, -0.02728405,\n",
       "       -0.51844776, -0.9557729 , -0.0447108 , -0.424039  ,  0.45476815,\n",
       "        0.5911588 ,  0.76276433, -0.3132864 , -0.04146839, -0.7162987 ,\n",
       "       -0.07241749, -1.371207  , -0.31391716, -0.73100954, -0.21320039,\n",
       "        1.0922103 ,  0.31125253,  0.4317013 ,  0.9374813 ,  0.19062869,\n",
       "       -0.36007923,  0.5329514 , -0.5572704 ,  0.33488792, -0.6640924 ,\n",
       "        0.47277847, -0.23200971,  1.7224989 ,  0.5872937 ,  0.08474042,\n",
       "       -1.3061486 , -0.07197918, -0.07680016,  0.8138374 ,  1.3192397 ,\n",
       "        0.12671499, -0.22916922,  0.9782795 ,  0.6477222 , -0.28950304,\n",
       "       -0.3767516 ,  0.28815356,  0.43184236, -0.19069389,  0.3739281 ,\n",
       "       -0.1409797 ,  0.33875376, -0.14300728,  0.6732585 , -0.6241671 ,\n",
       "        0.6535004 ,  0.2698578 ,  0.8960673 ,  0.47058603, -0.85397184,\n",
       "       -0.34458378,  0.43461263, -0.23304343, -0.63023025, -0.6338231 ,\n",
       "        0.5585066 , -0.6815549 ,  0.3013359 ,  0.03892016,  0.7766942 ,\n",
       "        0.7956397 ,  0.39256763,  0.29582497,  0.50663185,  0.47282147,\n",
       "        0.18485707,  0.38229188, -0.6922162 ,  1.388716  ,  0.24543473,\n",
       "       -0.05521877, -0.2237157 ,  0.6290598 , -0.42905018,  0.7544756 ,\n",
       "       -0.15668029,  0.2106686 , -0.28891003, -0.01996476,  0.00259322,\n",
       "       -0.6072628 , -0.01718482,  0.6117771 ,  0.12084786,  0.23105294],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft_kor[\"ㄴㅏㅁㄷㅗㅇㅅㅐㅇ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d8a482e0-7ec5-432c-8ac3-6f646df456ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.83176589012146, 'ㄴㅏㅁㅊㅣㄴ'),\n",
       " (0.8244485855102539, 'ㄷㅗㅇㅅㅐㅇ'),\n",
       " (0.8109169006347656, 'ㄴㅏㅁㅈㅏ-ㅊㅣㄴㄱㅜ-'),\n",
       " (0.7926679849624634, 'ㄴㅏㅁㅍㅕㄴ'),\n",
       " (0.7442033290863037, 'ㄴㅏㄴㅅㅐㅇ'),\n",
       " (0.7185494899749756, 'ㅊㅣㄴㄱㅜ-'),\n",
       " (0.7138498425483704, 'ㅊㅗ-ㄷㅡㅇㅅㅐㅇ'),\n",
       " (0.6991846561431885, 'ㄸㅏㄹㄹㅐㅁ'),\n",
       " (0.6507283449172974, 'ㅍㅕㅇㅅㅐㅇ'),\n",
       " (0.6504088044166565, 'ㅇㅕ-ㅈㅏ-ㅊㅣㄴㄱㅜ-')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 남동생 벡터와 가장 유사도가 높은 벡터 확인\n",
    "model_ft_kor.get_nearest_neighbors(word_to_jamo(\"남동생\"), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d8a1d58-1936-4d8b-b968-bf9cad05c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jamo_to_word() 함수 사용해서 자모 분해된 것을 단어로 복워해서 출력 \n",
    "def transform(word_sequence):\n",
    "    return [(jamo_to_word(word), similarity) for (similarity, word) in word_sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ff1be697-589c-4164-8966-2d5a8f985d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('남친', 0.83176589012146),\n",
       " ('동생', 0.8244485855102539),\n",
       " ('남자친구', 0.8109169006347656),\n",
       " ('남편', 0.7926679849624634),\n",
       " ('난생', 0.7442033290863037),\n",
       " ('친구', 0.7185494899749756),\n",
       " ('초등생', 0.7138498425483704),\n",
       " ('딸램', 0.6991846561431885),\n",
       " ('평생', 0.6507283449172974),\n",
       " ('여자친구', 0.6504088044166565)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(model_ft_kor.get_nearest_neighbors(word_to_jamo(\"남동생\"), k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "628d54eb-0aaa-48a7-8151-45fe7a058938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('음질', 0.7979444265365601),\n",
       " ('품질', 0.7903384566307068),\n",
       " ('땜질', 0.7696232795715332),\n",
       " ('찜질', 0.7372652292251587),\n",
       " ('질', 0.7172016501426697),\n",
       " ('화질', 0.6837594509124756),\n",
       " ('좀질', 0.6759639978408813),\n",
       " ('질질', 0.6579735279083252),\n",
       " ('질환', 0.6554113626480103),\n",
       " ('빗질', 0.6542677879333496)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(model_ft_kor.get_nearest_neighbors(word_to_jamo(\"고품질\"), k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9ceb708e-b320-45f2-afe9-66a8d4566324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('보풀', 0.7431467175483704),\n",
       " ('보질', 0.7429668307304382),\n",
       " ('음질', 0.7326189279556274),\n",
       " ('보폴', 0.7292374968528748),\n",
       " ('후질', 0.7229625582695007),\n",
       " ('화질', 0.7215917706489563),\n",
       " ('보이질', 0.7149317264556885),\n",
       " ('품질', 0.7018211483955383),\n",
       " ('재질', 0.6830146908760071),\n",
       " ('찜질', 0.6786102652549744)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(model_ft_kor.get_nearest_neighbors(word_to_jamo(\"보품질\"), k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "80f5867f-b373-416f-b447-2c090c9fd855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('반제품', 0.8767932057380676),\n",
       " ('재품', 0.8449320793151855),\n",
       " ('상품', 0.8114844560623169),\n",
       " ('명품', 0.7820002436637878),\n",
       " ('신제품', 0.7724065184593201),\n",
       " ('중품', 0.7699512243270874),\n",
       " ('리퍼제품', 0.7655267119407654),\n",
       " ('검품', 0.7478256225585938),\n",
       " ('파품', 0.7447069883346558),\n",
       " ('재고품', 0.7431009411811829)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(model_ft_kor.get_nearest_neighbors(word_to_jamo(\"제품\"), k=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bac821-4fb1-4958-846a-c7e0268ac100",
   "metadata": {},
   "source": [
    "#### FastText 임베딩 벡터는 오타와 노이즈 단어에 꽤 높은 대응을 진행\n",
    "- 딥러닝 모델 성능 향상 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d4b44f-34cb-4993-ae41-ac56b00e6fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_ENV",
   "language": "python",
   "name": "dlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
