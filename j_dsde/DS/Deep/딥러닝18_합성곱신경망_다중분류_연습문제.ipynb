{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8d0a8d-fe15-41ef-b049-d80ddfa5e7cf",
   "metadata": {},
   "source": [
    "## 합성곱신경망 이미지 다중분류 연습문제\n",
    "- fast-food 이미지 사용해서 \n",
    "- 버거, 도넛, 피자 3개 이미지 분류\n",
    "- fast-food-test-set을 검증용(0.8)과 테스트용(0.2)으로 분리해서 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d0b0cc-ddf0-4bbf-831c-0fbe9d0cd830",
   "metadata": {},
   "source": [
    "### 이미지 파일\n",
    "fast-food  \n",
    "- Buger (1500)  \n",
    "- Donut (1500)  \n",
    "- Pizza (1500)  \n",
    "\n",
    "fast-food-test-val (test/val 600/900)\n",
    "- Buger (1500)  \n",
    "- Donut (1500)  \n",
    "- Pizza (1500)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923502be",
   "metadata": {},
   "source": [
    "#### 배부된 폴더 data 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "707df137",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DIR = \"./fast-food/\"                   # 학습용 데이터셋\n",
    "VALIDATION_DIR = './fast-food-test-val/train'   # 검증용 데이터셋\n",
    "TEST_DIR = './fast-food-test-val/val'           # 테스트용 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "610719eb-5ed5-4ab8-b18a-4eca2671252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 모델 생성해서 확인\n",
    "# - 기본 : EarlyStopping/CheckPoint/Dropout/padding 아무것도 적용하지 않은 경우\n",
    "# - EarlyStopping/CheckPoint만 적용한 경우\n",
    "# - EarlyStopping/CheckPoint/Dropout 적용한 경우\n",
    "# - EarlyStopping/CheckPoint/Dropout/padding 적용한 경우 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5451d3d8-1f38-44e6-834f-4a64acbbc091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity=\"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b432b201-8662-49ba-a591-25f1623c3226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c9349b4-6d09-4b88-9679-76762e1322a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "# 검증용/테스트용 데이터 이미지 제너레이터\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e08147c-0a35-4ed4-8ce7-bcd5d973a4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 760 images belonging to 3 classes.\n",
      "Found 900 images belonging to 3 classes.\n",
      "Found 610 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# 제너레이터 객체 사용해서 이미지 조정\n",
    "\n",
    "# 학습용\n",
    "train_generator = training_datagen.flow_from_directory(\n",
    "    TRAINING_DIR,\n",
    "    target_size=(300, 300),\n",
    "    class_mode= \"categorical\", # 다중클래스 분류\n",
    ")\n",
    "\n",
    "# 검증용\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    VALIDATION_DIR,\n",
    "    target_size=(300, 300),\n",
    "    class_mode= \"categorical\", # 다중클래스 분류\n",
    ")\n",
    "\n",
    "# 테스트용\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(300, 300),\n",
    "    class_mode= \"categorical\", # 다중클래스 분류\n",
    ")\n",
    "\n",
    "\n",
    "# 조기종료, 체크포인트까지 적용해보고 (300, 300)을 (100, 100)으로 픽셀을 내리고,\n",
    "# 층을 하나 추가해볼 생각"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "503af566-b06b-4f2d-ac06-679eabd134c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82108\\anaconda3\\envs\\DLenv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\82108\\anaconda3\\envs\\DLenv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 1s/step - accuracy: 0.9332 - loss: 0.8138 - val_accuracy: 0.3333 - val_loss: 33.8478\n",
      "Epoch 2/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 852ms/step - accuracy: 0.9937 - loss: 0.2442 - val_accuracy: 0.3333 - val_loss: 9.7400\n",
      "Epoch 3/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.9947 - loss: 0.0769 - val_accuracy: 0.3333 - val_loss: 9.8608\n",
      "Epoch 4/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.9935 - loss: 0.0663 - val_accuracy: 0.3333 - val_loss: 13.5115\n",
      "Epoch 5/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 826ms/step - accuracy: 0.9936 - loss: 0.0593 - val_accuracy: 0.3333 - val_loss: 13.6220\n",
      "Epoch 6/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 816ms/step - accuracy: 0.9942 - loss: 0.0268 - val_accuracy: 0.3333 - val_loss: 14.8475\n",
      "Epoch 7/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 844ms/step - accuracy: 0.9970 - loss: 0.0090 - val_accuracy: 0.3333 - val_loss: 13.9074\n",
      "Epoch 8/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 856ms/step - accuracy: 0.9975 - loss: 0.0096 - val_accuracy: 0.3344 - val_loss: 19.9274\n",
      "Epoch 9/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 889ms/step - accuracy: 0.9980 - loss: 0.0045 - val_accuracy: 0.3333 - val_loss: 19.1126\n",
      "Epoch 10/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 869ms/step - accuracy: 0.9986 - loss: 0.0036 - val_accuracy: 0.3333 - val_loss: 39.0238\n",
      "Epoch 11/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 896ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.3344 - val_loss: 38.0200\n",
      "Epoch 12/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 835ms/step - accuracy: 1.0000 - loss: 7.4498e-05 - val_accuracy: 0.3367 - val_loss: 28.5414\n",
      "Epoch 13/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 902ms/step - accuracy: 1.0000 - loss: 7.1315e-05 - val_accuracy: 0.3333 - val_loss: 64.0709\n",
      "Epoch 14/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 902ms/step - accuracy: 1.0000 - loss: 2.3613e-04 - val_accuracy: 0.3356 - val_loss: 53.4339\n",
      "Epoch 15/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 856ms/step - accuracy: 1.0000 - loss: 4.4762e-06 - val_accuracy: 0.3344 - val_loss: 54.4765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26ecda0eae0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Rescaling, Dropout\n",
    "\n",
    "# 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (3,3), input_shape=(300, 300, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Conv2D(32, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Conv2D(64, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation= \"relu\"))\n",
    "model.add(Dense(3, activation='softmax'))   # 다중 클래스 분류 (가위/바위/보)\n",
    "\n",
    "# 컴파일 \n",
    "model.compile(loss='categorical_crossentropy', # 다중 분류\n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 학습 \n",
    "model.fit(train_generator,  # 학습용\n",
    "         epochs=15,\n",
    "         validation_data=validation_generator) # 검증용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f99e06a-817a-49e3-8cd1-c67a4159c8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 373ms/step - accuracy: 0.3510 - loss: 54.5979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[57.221195220947266, 0.3508196771144867]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터 검증\n",
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c78c1-1a48-417a-bc26-07973c89560c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d56f48b7-d8b9-4f05-9098-298e1baccda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 869ms/step - accuracy: 1.0000 - loss: 1.0951e-06 - val_accuracy: 0.3344 - val_loss: 58.6012\n",
      "Epoch 2/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 894ms/step - accuracy: 1.0000 - loss: 1.2869e-06 - val_accuracy: 0.3356 - val_loss: 55.6627\n",
      "Epoch 3/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 930ms/step - accuracy: 1.0000 - loss: 1.5317e-06 - val_accuracy: 0.3344 - val_loss: 61.9692\n",
      "Epoch 4/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 881ms/step - accuracy: 1.0000 - loss: 6.7756e-07 - val_accuracy: 0.3344 - val_loss: 59.8280\n"
     ]
    }
   ],
   "source": [
    "# EarlyStopping과 Check Point 설정\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "check_pointer = ModelCheckpoint(\"./model/fast_food_best_model.keras\")\n",
    "early_stopping_callback = EarlyStopping(patience=2, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs =15,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[check_pointer, early_stopping_callback] ,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38803fdb-3428-4cb1-9491-d3e19d2686ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 169ms/step - accuracy: 0.3517 - loss: 57.3263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[58.49161148071289, 0.3508196771144867]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터 검증\n",
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91889ab-55ec-480f-b490-4f18be745836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0102205-443e-4e81-8869-ee708bfd0a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf873c98-6d7a-470e-821f-c5acfcaca92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 잘 예측하는지 확인\n",
    "\n",
    "# rsp_name = ['보', '바위', '가위']\n",
    "\n",
    "# for n in sample_images:\n",
    "#     # 이미지 출력\n",
    "#     plt.imshow(mpimg.imread(n))\n",
    "#     plt.show()\n",
    "    \n",
    "#     # 이미지 처리\n",
    "#     img = tf.keras.utils.load_img(n, target_size=(300, 300))\n",
    "#     x = tf.keras.utils.img_to_array(img)\n",
    "#     x = np.expand_dims(x, axis=0) # (1, 300, 300, 3) 형태가 되도록\n",
    "#     # 클래스 예측\n",
    "#     classes = model.predict(x)\n",
    "#     print(classes)   \n",
    "#     idx = np.argmax(classes[0])\n",
    "#     print('idx : ', idx)\n",
    "#     print(n + '는 {}입니다.'.format(rsp_name[idx]))\n",
    "    \n",
    "#     print('=============================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa089630-9bbc-4fe2-9a51-a71e2d788506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 760 images belonging to 3 classes.\n",
      "Found 900 images belonging to 3 classes.\n",
      "Found 610 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# 제너레이터 객체 사용해서 이미지 조정\n",
    "\n",
    "# 학습용\n",
    "train_generator = training_datagen.flow_from_directory(\n",
    "    TRAINING_DIR,\n",
    "    target_size=(100, 100),\n",
    "    class_mode= \"categorical\", # 다중클래스 분류\n",
    ")\n",
    "\n",
    "# 검증용\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    VALIDATION_DIR,\n",
    "    target_size=(100, 100),\n",
    "    class_mode= \"categorical\", # 다중클래스 분류\n",
    ")\n",
    "\n",
    "# 테스트용\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(100, 100),\n",
    "    class_mode= \"categorical\", # 다중클래스 분류\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8dd59be-2200-481b-a865-1c80f5b9ce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 128ms/step - accuracy: 0.8403 - loss: 0.2148 - val_accuracy: 0.3333 - val_loss: 6.1896\n",
      "Epoch 2/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.9954 - loss: 0.0396 - val_accuracy: 0.3333 - val_loss: 8.2088\n",
      "Epoch 3/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9963 - loss: 0.0384 - val_accuracy: 0.3333 - val_loss: 8.0015\n",
      "Epoch 4/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.9948 - loss: 0.0619 - val_accuracy: 0.3333 - val_loss: 7.4860\n",
      "Epoch 5/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.9977 - loss: 0.0248 - val_accuracy: 0.3333 - val_loss: 6.9184\n",
      "Epoch 6/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9973 - loss: 0.0235 - val_accuracy: 0.3333 - val_loss: 6.1879\n",
      "Epoch 7/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9980 - loss: 0.0151 - val_accuracy: 0.3333 - val_loss: 6.6380\n",
      "Epoch 8/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9983 - loss: 0.0128 - val_accuracy: 0.3333 - val_loss: 6.0622\n",
      "Epoch 9/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9985 - loss: 0.0089 - val_accuracy: 0.3333 - val_loss: 6.6877\n",
      "Epoch 10/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9958 - loss: 0.0187 - val_accuracy: 0.3333 - val_loss: 9.7249\n",
      "Epoch 11/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9955 - loss: 0.0140 - val_accuracy: 0.3333 - val_loss: 11.2761\n",
      "Epoch 12/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9968 - loss: 0.0142 - val_accuracy: 0.3333 - val_loss: 8.6257\n",
      "Epoch 13/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9972 - loss: 0.0078 - val_accuracy: 0.3333 - val_loss: 6.2681\n",
      "Epoch 14/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9979 - loss: 0.0088 - val_accuracy: 0.3333 - val_loss: 14.8557\n",
      "Epoch 15/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9941 - loss: 0.0137 - val_accuracy: 0.3333 - val_loss: 14.3192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26f1158be60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Rescaling, Dropout\n",
    "\n",
    "# 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (3,3), input_shape=(100, 100, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Conv2D(32, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Conv2D(64, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Conv2D(128, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "model.add(Conv2D(256, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation= \"relu\"))\n",
    "model.add(Dense(3, activation='softmax'))   # 다중 클래스 분류 (가위/바위/보)\n",
    "\n",
    "# 컴파일 \n",
    "model.compile(loss='categorical_crossentropy', # 다중 분류\n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# # 학습 \n",
    "model.fit(train_generator,  # 학습용\n",
    "         epochs=15,\n",
    "         validation_data=validation_generator) # 검증용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e35ad167-faeb-4cc8-b0cf-e3089dc48060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.3599 - loss: 14.3823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15.317941665649414, 0.3442623019218445]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 테스트 데이터 검증\n",
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc41d3c8-d69c-4f6f-ba64-9cc91c833c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9935 - loss: 0.0074 - val_accuracy: 0.3333 - val_loss: 30.4606\n",
      "Epoch 2/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.9982 - loss: 0.0048 - val_accuracy: 0.3333 - val_loss: 16.7377\n",
      "Epoch 3/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9938 - loss: 0.0065 - val_accuracy: 0.3333 - val_loss: 20.0399\n",
      "Epoch 4/15\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9976 - loss: 0.0056 - val_accuracy: 0.3889 - val_loss: 29.2210\n"
     ]
    }
   ],
   "source": [
    "# EarlyStopping과 Check Point 설정\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "check_pointer = ModelCheckpoint(\"./model/fast_food_best_model100.keras\")\n",
    "early_stopping_callback = EarlyStopping(patience=2, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs =15,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[check_pointer, early_stopping_callback] ,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4d235f1-a3fb-452b-adb4-2529d9f6c230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.3349 - loss: 18.8593 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[18.058605194091797, 0.3442623019218445]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터 검증\n",
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f420e3-25a1-4da1-853b-25c63b95d92c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca2a39-0a34-41ad-a2c5-40e691a8d602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb75a9d5-fd78-4277-92e5-6b218019202c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_ENV",
   "language": "python",
   "name": "dlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
